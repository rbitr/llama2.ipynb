{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c54d3f",
   "metadata": {},
   "source": [
    "## llama2.ipynb\n",
    "\n",
    "This is a direct python translation of Andrej Karpathy's llama2.c: https://github.com/karpathy/llama2.c/tree/master\n",
    "\n",
    "The purpose is for learning, and to show that \"AI\" is nothing to fear. This couldn't possibly harm you, nor could it take over the world or cause human extinction or anything like that. It's just a computer program. People claiming this is something to fear are mistaken.\n",
    "\n",
    "This is (eventually) meant to show a non-specialist audience what \"AI\" really looks like.\n",
    "\n",
    "This is the same model (only smaller) that was released by Meta (facebook) recently, and uses a similar architecture to all the modern language models like GPT. It uses weights that were trained on a dataset of children's stories, so it only writes short children's stories. You can go to the link above to learn more about it.\n",
    "\n",
    "The program being in a jupyter notebook makes it easy to play with, but means the actual loop where the program runs is at the bottom. The upper part takes care of setting up the model and reading in the weights. Scroll to the bottom to see example output.\n",
    "\n",
    "I translated this from the C-program (that used all loops for linear algebra) and made only minimal changes so far to make sure I could get it working. I may clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a85248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the commands below to download the model (60 MB) and vocabulary if you don't have them\n",
    "\n",
    "#!wget https://github.com/karpathy/llama2.c/raw/master/tokenizer.bin\n",
    "#!wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9e1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "# np makes matrix multiplication and manipulation easier, struct is for reading in the weights file\n",
    "# sys is for printing. There are no dependencies of any machine learning frameworks\n",
    "\n",
    "import numpy as np\n",
    "import struct\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37ecd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to read in weights\n",
    "\n",
    "def get_weights(buffer, size, offset, b=4):\n",
    "    w_size = np.prod(size)\n",
    "    w = np.array(struct.unpack(f\"{w_size}f\", buffer[offset:(offset+w_size*b)]))\n",
    "    w = w.reshape(size)\n",
    "    return w, offset+w_size*b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1410a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the example model is a small llm trained on a dataset of stories by Andrej Karpathy\n",
    "# since the model is already trained, this code just reads it in\n",
    "# the weights in the model file are used as multiplying factors in the model\n",
    "\n",
    "model = \"stories15M.bin\"\n",
    "\n",
    "with open(model, mode='rb') as file: # b is important -> binary\n",
    "    fileContent = file.read()\n",
    "\n",
    "# the model file begings with 7 integers of configuration data\n",
    "# dim, hidden_dim, n_layers, n_heads, n_kv_heads, vocab_size, seq_len\n",
    "config = struct.unpack(\"iiiiiii\", fileContent[:28])\n",
    "dim, hidden_dim, n_layers, n_heads, n_kv_heads, vocab_size, seq_len = config\n",
    "\n",
    "# the sizes of the different types of layer, to be used in reading them in\n",
    "emb_size = (vocab_size,dim)\n",
    "rmsnorm_size = (n_layers, dim)\n",
    "layer_size = (n_layers,dim,dim)\n",
    "ffn_13_size = (n_layers,hidden_dim,dim) \n",
    "ffn_2_size = (n_layers, dim, hidden_dim) \n",
    "rmsfinal_size = (dim,)\n",
    "head_size = dim // n_heads\n",
    "cis_size = (seq_len, head_size//2)\n",
    "\n",
    "# read in weights and use them to create numpy arrays for use in multiplication\n",
    "weight_dict = {}\n",
    "token_embedding_table, offset = get_weights(fileContent,emb_size,28) \n",
    "weight_dict['rms_att_weight'], offset = get_weights(fileContent,rmsnorm_size,offset) \n",
    "weight_dict['wq'], offset = get_weights(fileContent,layer_size,offset) \n",
    "weight_dict['wk'], offset = get_weights(fileContent,layer_size,offset)\n",
    "weight_dict['wv'], offset = get_weights(fileContent,layer_size,offset)\n",
    "weight_dict['wo'], offset = get_weights(fileContent,layer_size,offset)\n",
    "weight_dict['rms_ffn_weight'], offset = get_weights(fileContent,rmsnorm_size,offset) \n",
    "weight_dict['w1'], offset = get_weights(fileContent,ffn_13_size,offset)\n",
    "weight_dict['w2'], offset = get_weights(fileContent,ffn_2_size,offset)\n",
    "weight_dict['w3'], offset = get_weights(fileContent,ffn_13_size,offset)\n",
    "weight_dict['rms_final_weight'], offset = get_weights(fileContent,rmsfinal_size,offset)\n",
    "weight_dict['freq_cis_real'], offset = get_weights(fileContent,cis_size,offset)\n",
    "weight_dict['freq_cis_imag'], offset = get_weights(fileContent,cis_size,offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e1d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "# normalize a vector and multiply by weights\n",
    "def rmsnorm(x,w):\n",
    "    xn = np.sqrt(np.dot(x,x)/x.shape[0]+1e-5)\n",
    "    return x*w/(xn)\n",
    "\n",
    "# convert scores to probabilities\n",
    "def softmax(x,size):\n",
    "    \n",
    "    mv = max(x[:size])\n",
    "    xi = np.exp(x[:size]-mv)\n",
    "    \n",
    "    return np.concatenate([xi/sum(xi),0*x[size:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f39fd",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "In the cell below is the \"transformer\" that everybody is talking about, this is what's created a new hype wave in AI. It's essentially a lookup of the next word based on past words, with some standard tricks from deep learning thrown in and parameters that can be optimized to make the lookup match well with real sentences.\n",
    "\n",
    "It's not necessary to understand it all, just that it's a short computer program that usefully processes input text to determine a likely next word.\n",
    "\n",
    "I recommend this blog post for a better description of how it works: https://jaykmody.com/blog/gpt-from-scratch/\n",
    "\n",
    "Bottom line, \"attention\" is what performs the lookup $$A = \\mbox{softmax}(\\frac{QK^T}{\\sqrt{(d_k)}})V$$\n",
    "where in an LLM $Q$, $K$ and $V$ are projections of the input text. The rest is just some programming around this equation to make it happen in a way that works robustly in a big deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ac6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token is the current word\n",
    "# pos is the position within the text\n",
    "# s keeps the past words\n",
    "# w is the weights\n",
    "# the token embedding table converts words to vectors for processing in the transformer\n",
    "def transformer(token, pos, config, s, w, token_embedding_table=token_embedding_table):\n",
    "    \n",
    "    #just unpack the config\n",
    "    dim, hidden_dim, n_layers, n_heads, n_kv_heads, vocab_size, seq_len = config\n",
    "    head_size = dim // n_heads\n",
    "    \n",
    "    # convert the word to a vector\n",
    "    x = token_embedding_table[token]\n",
    "    \n",
    "    # for encoding the position\n",
    "    freq_cis_real_row = weight_dict[\"freq_cis_real\"][pos]\n",
    "    freq_cis_imag_row = weight_dict[\"freq_cis_imag\"][pos]\n",
    "    \n",
    "    for l in range(n_layers): # probably a better way...\n",
    "        \n",
    "        # a bunch of matric multiplications to mix in the weights\n",
    "        xb = rmsnorm(x, weight_dict[\"rms_att_weight\"][l])\n",
    "        \n",
    "        # these are the vectors for the key-value lookup (\"attention\")\n",
    "        q = np.matmul(xb,weight_dict[\"wq\"][l].T)\n",
    "        k = np.matmul(xb,weight_dict[\"wk\"][l].T)\n",
    "        v = np.matmul(xb,weight_dict[\"wv\"][l].T)\n",
    "        \n",
    "\n",
    "        # position encodeing (RoPE)\n",
    "        for h in range(n_heads):\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(0,head_size,2): # n_heads is 6, head_size is 48, total 288\n",
    "                 \n",
    "                q0 = q[h*head_size+i]  # just copy the c code, we can vectorize and do all at once after\n",
    "                q1 = q[h*head_size+i+1]\n",
    "                k0 = k[h*head_size+i]\n",
    "                k1 = k[h*head_size+i+1]\n",
    "                fcr = freq_cis_real_row[i//2]\n",
    "                fci = freq_cis_imag_row[i//2]\n",
    "                q[h*head_size+i] = q0 * fcr - q1 * fci\n",
    "                q[h*head_size+i+1] = q0 * fci + q1 * fcr\n",
    "                k[h*head_size+i] = k0 * fcr - k1 * fci\n",
    "                k[h*head_size+i+1] = k0 * fci + k1 * fcr\n",
    " \n",
    "    # saves it because it uses it for past timesteps\n",
    "    \n",
    "        s[\"key_cache\"][l][pos] = k\n",
    "        s[\"value_cache\"][l][pos] = v\n",
    "    \n",
    "        # attention\n",
    "        xb = np.zeros(dim)\n",
    "        for h in range(n_heads):\n",
    "    \n",
    "            q_t = q[h*head_size:(h+1)*head_size] #q[h*headsize] is the start\n",
    "        \n",
    "            for t in range(pos+1):\n",
    "        \n",
    "                k_t = s[\"key_cache\"][l][t][h*head_size:(h+1)*head_size]\n",
    "                score = np.dot(q_t,k_t)/np.sqrt(head_size)\n",
    "                s[\"att\"][h][t] = score\n",
    "            \n",
    "                \n",
    "            s[\"att\"][h] = softmax(s[\"att\"][h],pos+1)\n",
    "            \n",
    "            xbh = np.zeros(head_size)\n",
    "            \n",
    "            for t in range(pos+1):\n",
    "                \n",
    "                v_t = s[\"value_cache\"][l][t][h*head_size:(h+1)*head_size]\n",
    "              \n",
    "                a = s[\"att\"][h][t]\n",
    "                xbh += a*v_t\n",
    "               \n",
    "        \n",
    "            xb[h*head_size:(h+1)*head_size] = xbh\n",
    "            \n",
    "\n",
    "        # more weights + residual\n",
    "        xb2 = np.matmul(xb, weight_dict[\"wo\"][l].T)\n",
    "        x = x + xb2\n",
    "        \n",
    "        xb = rmsnorm(x,weight_dict[\"rms_ffn_weight\"][l])\n",
    "        #print(xb.shape)\n",
    "        #print(weight_dict[\"w1\"][l].shape)\n",
    "        \n",
    "        hb = np.matmul(xb, weight_dict[\"w1\"][l].T)\n",
    "        hb2 = np.matmul(xb, weight_dict[\"w3\"][l].T)\n",
    "        \n",
    "        hb = hb*(1/(1+np.exp(-hb)))\n",
    "        \n",
    "        hb = hb*hb2\n",
    "        \n",
    "        xb = np.matmul(hb, weight_dict[\"w2\"][l].T)\n",
    "        \n",
    "        x += xb\n",
    "\n",
    "\n",
    "    x = rmsnorm(x, weight_dict[\"rms_final_weight\"])\n",
    "    logits = np.matmul(x, token_embedding_table.T) # shared weights\n",
    "    \n",
    "    \n",
    "    return x, logits\n",
    "   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20bcb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from a vector of probabilities in proportion to each \n",
    "# there's a numpy function I belive\n",
    "def sample(p):\n",
    "    r = np.random.random()\n",
    "    cdf = 0\n",
    "    for i, v in enumerate(p):\n",
    "        cdf+=v\n",
    "        if r<cdf: return i\n",
    "        \n",
    "    return len(p)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d63e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read in the vocabulary data that turns each word into a number for manipulation\n",
    "def read_token(vc, p):\n",
    "    # read a float score\n",
    "    score = struct.unpack(\"f\",vc[p:(p+4)])\n",
    "    # read length\n",
    "    tok_len = struct.unpack(\"i\",vc[(p+4):(p+8)])[0]\n",
    "    #print(tok_len)\n",
    "    # read the string\n",
    "    token = struct.unpack(f\"{tok_len}s\",vc[(p+8):(p+8+tok_len)])\n",
    "    return score[0], token[0].decode(), p+8+tok_len\n",
    "\n",
    "# file that was downloaded\n",
    "vocab_file = \"tokenizer.bin\"\n",
    "\n",
    "with open(vocab_file, mode='rb') as file: # b is important -> binary\n",
    "    vc = file.read()\n",
    "\n",
    "# int for max_token_length\n",
    "max_token_length = struct.unpack(\"i\",vc[:4])\n",
    "\n",
    "\n",
    "vocab = []\n",
    "vocab_scores = []\n",
    "p = 4\n",
    "for i in range(vocab_size):\n",
    "    s, t, p = read_token(vc,p)\n",
    "    vocab.append(t)\n",
    "    vocab_scores.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca18b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup a word's position in the vocabulary file\n",
    "def lookup(s,vocab=vocab):\n",
    "    for i, word in enumerate(vocab):\n",
    "        if s==word: return i\n",
    "    return -1\n",
    "\n",
    "# encode a text string into tokens (words or parts of words)\n",
    "def bpe_encode(text, vocab=vocab, vocab_scores=vocab_scores):\n",
    "    \n",
    "    tokens = [lookup(c) for c in text]\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        running_merge = [vocab[tokens[i]]+vocab[tokens[i+1]] for i in range(len(tokens)-1)]\n",
    "               \n",
    "        best_id = -1\n",
    "        best_score = -1e10\n",
    "        \n",
    "        for i, m in enumerate(running_merge):\n",
    "            ind = lookup(m)\n",
    "            if ind>0:\n",
    "                score = vocab_scores[ind]\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_id = i\n",
    "        \n",
    "        #print(\"Best id\", best_id)\n",
    "        #print(\"token there:\", running_merge[best_id])\n",
    "        #print(\"position in vocab:\", lookup(running_merge[best_id]))\n",
    "        \n",
    "        if best_id == -1: break\n",
    "        \n",
    "        tokens[best_id] = lookup(running_merge[best_id])\n",
    "        \n",
    "        tokens.pop(best_id+1)\n",
    "        #print \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4cd7a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the cache\n",
    "state_dict = {}\n",
    "state_dict[\"key_cache\"] = np.zeros((n_layers,seq_len,dim))\n",
    "state_dict[\"value_cache\"] = np.zeros((n_layers,seq_len,dim))\n",
    "state_dict[\"att\"] = np.zeros((n_heads,seq_len)) # could be dropped and moved inside the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121c9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an old house, a little girl named Amy was playing with her toys. She loved to play all day long. Her mom called her from the house, \"Amy, come and help me repair your broken fork.\"\n",
      "Amy went to her mom and said, \"Mom, I can't fix your fork. It's broken.\" Her mom looked at the fork and said, \"Amy, I can fix it for you. You need to improve your work by being a good helper.\"\n",
      "Amy practiced being a good girl all day. She helped her mom sort the dishes, cleaned the floor, and even helped her dad in the garden. As she worked, she remembered to be good at fixing things. She kept adding to her fix skills until it was time for dinner. When dinner was ready, Amy's family was so happy. They all sat down and ate together. The fixers were so proud of Amy that they gave her a big hug. Amy felt happy that she could help and improve her family's home. The end.\n",
      "<s>\n",
      " Once upon a time, there was a little girl named Lily. One night, she was scared because she heard a monster in her dream"
     ]
    }
   ],
   "source": [
    "# this is where the model runs. \n",
    "\n",
    "# inputs\n",
    "temperature = 1 # zero makes it deterministic, bigger numbers increase the variability\n",
    "steps = 256 # size of the text generated\n",
    "prompt = \"In an old house\"\n",
    "\n",
    "# start with token 1 which is the \"beginning of sentence\" token and at position zero\n",
    "token = 1\n",
    "pos = 0\n",
    "\n",
    "prompt_tokens = bpe_encode(prompt)\n",
    "\n",
    "while(pos < steps):\n",
    "    \n",
    "    # at each step, the next token is calculated from the previous ones\n",
    "    x, logits = transformer(token, pos, config, state_dict, weight_dict)\n",
    "    \n",
    "    if pos < len(prompt_tokens):\n",
    "        \n",
    "        next_t = prompt_tokens[pos]\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        if temperature == 0:\n",
    "        \n",
    "            next_t = np.argmax(logits)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            logits = softmax(logits/temperature,len(logits))\n",
    "            next_t = sample(logits)\n",
    "        \n",
    "    token_str = vocab[next_t]\n",
    "    print(token_str,end=\"\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    token = next_t\n",
    "    pos+=1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b3e50",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "You can see example output above. This model has 15 Million paremeters (weights) so it is relatively simply but it still writes a coherent story by predicting next words. Models like GPT4 have hundreds of billions or trillions of parameters, which is why they appear so coherent and real. Architectually they are the same as this program, just scaled up to have a better model of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52d21b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
